# Workflow: Performance Benchmarks
#
# Purpose: Run performance benchmarks to track generation speed
# and detect performance regressions.
#
# Trigger: On push and pull requests to main/master branches.

name: Benchmarks

permissions:
  contents: read

on:
  push:
    branches: [ main, master ]
  pull_request:
    branches: [ main, master ]

jobs:
  benchmark:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v6
        with:
          lfs: true

      - name: Install uv
        uses: astral-sh/setup-uv@v7
        with:
          version: "0.9.26"
          python-version: "3.12"

      - name: Install dependencies
        run: |
          make install

      - name: Install benchmark dependencies
        run: |
          .venv/bin/uv pip install pytest-benchmark>=5.1.0

      - name: Run benchmarks
        run: |
          .venv/bin/python -m pytest tests/benchmarks/ \
            --benchmark-only \
            --benchmark-json=benchmark-results.json \
            --benchmark-columns=min,max,mean,stddev,rounds

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: benchmark-results.json
          retention-days: 30

      - name: Display benchmark summary
        run: |
          echo "## Benchmark Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Benchmark results have been saved as an artifact." >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "View the detailed JSON results in the uploaded artifact." >> $GITHUB_STEP_SUMMARY
